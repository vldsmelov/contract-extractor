name: contract_extractor

services:
  ollama:
    image: ollama/ollama:latest
    container_name: contract-extractor-ollama
    runtime: nvidia
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
    healthcheck:
      test: ["CMD-SHELL", "OLLAMA_HOST=127.0.0.1:11434 ollama list >/dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 30
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: ["gpu"]

  ollama-pull:
    image: curlimages/curl:8.10.1
    container_name: contract-extractor-ollama-pull
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      - OLLAMA_API=http://ollama:11434
      - OLLAMA_MODEL=${MODEL:-krith/qwen2.5-32b-instruct:IQ4_XS}
    entrypoint:
      - /bin/sh
      - -lc
      - |
        echo "[pull] waiting for $${OLLAMA_API} ..."
        for i in $$(seq 1 120); do
          if curl -fsS $${OLLAMA_API}/api/tags >/dev/null; then
            echo "[pull] server is up"; break
          fi
          sleep 2
        done
        echo "[pull] start pull: $${OLLAMA_MODEL}"
        curl -fsS -X POST $${OLLAMA_API}/api/pull \
          -H 'Content-Type: application/json' \
          -d '{"name":"'"$${OLLAMA_MODEL}"'"}' \
          | sed -u 's/^/[pull] /'
        echo "[pull] done"
    restart: "no"

  api:
    image: contract-extractor/api:dev
    build:
      context: ./api
      dockerfile: dockerfile
      args:
        BASE_IMAGE: ${API_BASE_IMAGE:-contract-extractor/api-base:cu130}
    container_name: contract-extractor-api
    environment:
      - OLLAMA_HOST=http://ollama:11434
      - MODEL=${MODEL:-krith/qwen2.5-32b-instruct:IQ4_XS}
      - TEMPERATURE=${TEMPERATURE:-0.1}
      - MAX_TOKENS=${MAX_TOKENS:-1024}
      - NUMERIC_TOLERANCE=${NUMERIC_TOLERANCE:-0.01}
      - USE_LLM=${USE_LLM:-true}
      - API_PORT=${API_PORT:-8080}
    depends_on:
      ollama:
        condition: service_healthy
      ollama-pull:
        condition: service_completed_successfully
    volumes:
      - ./api:/workspace/api
    working_dir: /workspace/api
    command: uvicorn app.main:app --host 0.0.0.0 --port 8080 --reload --log-level info
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://127.0.0.1:8080/healthz >/dev/null || exit 1"]
      interval: 15s
      timeout: 5s
      retries: 20
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    restart: unless-stopped

volumes:
  ollama_models:
    name: contract_extractor_ollama_models
